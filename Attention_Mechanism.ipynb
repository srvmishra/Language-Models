{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1qaI8yf7tJY+V2mMD7nkT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srvmishra/Language-Models/blob/main/Attention_Mecchanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "5C68e0B49x_V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-acocQuPG5Jm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `SelfAttentionV1`: Simple Self Attention with weights as `nn.Parameter`"
      ],
      "metadata": {
        "id": "LuXiWDX09z5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionV1(nn.Module):\n",
        "  def __init__(self, d_in, d_out):\n",
        "    super(SelfAttentionV1, self).__init__()\n",
        "    self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "    self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
        "    self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    x -> torch.tensor, shape: [batch_size, d_in]\n",
        "    '''\n",
        "    query = x @ self.W_query\n",
        "    key = x @ self.W_key\n",
        "    value = x @ self.W_value\n",
        "    attention_scores = query @ key.T\n",
        "    attention_weights = torch.softmax(attention_scores/value.shape[-1] ** 0.5,\n",
        "                                      dim=-1)\n",
        "    context_vector = attention_weights @ value\n",
        "    return context_vector\n",
        "\n",
        "  def set_weights(self, W_query, W_key, W_value):\n",
        "    self.W_query = nn.Parameter(W_query)\n",
        "    self.W_key = nn.Parameter(W_key)\n",
        "    self.W_value = nn.Parameter(W_value)"
      ],
      "metadata": {
        "id": "GxVjqXuiNRzU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `SelfAttentionV2`: Simple Self Attention with weights as `nn.Linear` with `bias=False`"
      ],
      "metadata": {
        "id": "TnQCSwYu96Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionV2(nn.Module):\n",
        "  def __init__(self, d_in, d_out):\n",
        "    super(SelfAttentionV2, self).__init__()\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    x -> torch.tensor, shape: [batch_size, d_in]\n",
        "    '''\n",
        "    query = self.W_query(x)\n",
        "    key = self.W_key(x)\n",
        "    value = self.W_value(x)\n",
        "    attention_scores = query @ key.T\n",
        "    attention_weights = torch.softmax(attention_scores/value.shape[-1] ** 0.5,\n",
        "                                      dim=-1)\n",
        "    context_vector = attention_weights @ value\n",
        "    return context_vector"
      ],
      "metadata": {
        "id": "EkLC7XMSNRhh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking outputs for both Self Attention implementations"
      ],
      "metadata": {
        "id": "QSB_eq7F-D45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor([[0.43, 0.15, 0.89],\n",
        "                       [0.55, 0.87, 0.66],\n",
        "                       [0.57, 0.85, 0.64],\n",
        "                       [0.22, 0.58, 0.33],\n",
        "                       [0.77, 0.25, 0.10],\n",
        "                       [0.05, 0.80, 0.55]])\n",
        "\n",
        "d_in = 3\n",
        "d_out = 2\n",
        "\n",
        "torch.manual_seed(123)\n",
        "sa_v1 = SelfAttentionV1(d_in, d_out)\n",
        "sa_v1_outs = sa_v1(inputs)\n",
        "print('Self Attention V1 outputs')\n",
        "print(sa_v1_outs)\n",
        "\n",
        "sa_v2 = SelfAttentionV2(d_in, d_out)\n",
        "sa_v2_outs = sa_v2(inputs)\n",
        "print('Self Attention V2 outputs')\n",
        "print(sa_v2_outs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Gy0rD74O_-o",
        "outputId": "8fdec25e-238b-4404-ce87-68d91b77917a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self Attention V1 outputs\n",
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n",
            "Self Attention V2 outputs\n",
            "tensor([[0.5085, 0.3508],\n",
            "        [0.5084, 0.3508],\n",
            "        [0.5084, 0.3506],\n",
            "        [0.5074, 0.3471],\n",
            "        [0.5076, 0.3446],\n",
            "        [0.5077, 0.3493]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Self Attention Weight Shapes')\n",
        "print('Query: V1: ', sa_v1.W_query.shape, ' V2: ', sa_v2.W_query.weight.shape)\n",
        "print('Key: V1: ', sa_v1.W_key.shape, ' V2: ', sa_v2.W_key.weight.shape)\n",
        "print('Value: V1: ', sa_v1.W_value.shape, ' V2: ', sa_v2.W_value.weight.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPRz8QE4Qx5u",
        "outputId": "abad8fa7-d734-40d9-b695-4b3581f2da05"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self Attention Weight Shapes\n",
            "Query: V1:  torch.Size([3, 2])  V2:  torch.Size([2, 3])\n",
            "Key: V1:  torch.Size([3, 2])  V2:  torch.Size([2, 3])\n",
            "Value: V1:  torch.Size([3, 2])  V2:  torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transferring weights from `SelfAttentionV2` -> `SelfAttentionV1` and matching outputs"
      ],
      "metadata": {
        "id": "y2xApCgm-Jvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_query = sa_v2.W_query.weight.T\n",
        "W_key = sa_v2.W_key.weight.T\n",
        "W_value = sa_v2.W_value.weight.T\n",
        "\n",
        "sa_v1.set_weights(W_query, W_key, W_value)\n",
        "sa_v1_outs_new = sa_v1(inputs)\n",
        "print(sa_v1_outs_new)\n",
        "print((sa_v1_outs_new == sa_v2_outs).all())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sD28Vwr3SjGt",
        "outputId": "dfc5e74e-fcfa-47c9-8623-c47935f922a2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5085, 0.3508],\n",
            "        [0.5084, 0.3508],\n",
            "        [0.5084, 0.3506],\n",
            "        [0.5074, 0.3471],\n",
            "        [0.5076, 0.3446],\n",
            "        [0.5077, 0.3493]], grad_fn=<MmBackward0>)\n",
            "tensor(True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `CausalSelfAttention`: Self Attention with Causal Masking"
      ],
      "metadata": {
        "id": "WVbeN5vL-csf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In attention masking, why do we add $-\\infty$ to the raw attention scores before computing the softmax? We could also zero out the attention weights after softmax above the diagonal and then rescale each row."
      ],
      "metadata": {
        "id": "n19L22FEU8CG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two ways to include dropout in attention scores - to prevent too much dependence on any single position for attention computation:\n",
        "1. directly apply to the attention weight matrix\n",
        "2. apply to the context vector after multiplying attention weight with value vector\n",
        "\n",
        "after applying the dropout, the resulting weights/vectors are scaled so that the overall logits stay consistent during training and inference. note that inference does not use dropouts. it is only used during training."
      ],
      "metadata": {
        "id": "Chc4qojLWemf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, drop_rate, qkv_bias=False):\n",
        "    super(CausalSelfAttention, self).__init__()\n",
        "    self.d_out = d_out\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(drop_rate)\n",
        "\n",
        "    # we dont have to worry about placing tensors separately on device, so we\n",
        "    # use register_buffer\n",
        "    self.register_buffer('mask',\n",
        "                         torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    x -> torch.tensor, shape: [num_sequences, num_tokens, d_in]\n",
        "    '''\n",
        "    query = self.W_query(x)\n",
        "    key = self.W_key(x)\n",
        "    value = self.W_value(x)\n",
        "\n",
        "    attention_scores = query @ key.transpose(-1, -2)\n",
        "\n",
        "    # in place operation as function ends with _\n",
        "    # max length is context length, but sequence only has num_tokens tokens\n",
        "    attention_scores.masked_fill_(self.mask.bool()[:attention_scores.shape[1], :attention_scores.shape[1]], -torch.inf)\n",
        "    attention_weights = torch.softmax(attention_scores/value.shape[-1] ** 0.5, dim=-1)\n",
        "    drop_attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "    context_vector = drop_attention_weights @ value\n",
        "    return context_vector"
      ],
      "metadata": {
        "id": "dANi7WngVOON"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.stack([inputs, inputs], dim=0)\n",
        "print(batch)\n",
        "print(batch.shape)\n",
        "ca = CausalSelfAttention(d_in, d_out, batch.shape[1], 0.0)\n",
        "ca_outputs = ca(batch)\n",
        "print('Causal Attention Outputs')\n",
        "print(ca_outputs)\n",
        "print(ca_outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMVR2IN7cNd7",
        "outputId": "0db16ff8-d540-4c8b-e984-e46e060ed9bc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.4300, 0.1500, 0.8900],\n",
            "         [0.5500, 0.8700, 0.6600],\n",
            "         [0.5700, 0.8500, 0.6400],\n",
            "         [0.2200, 0.5800, 0.3300],\n",
            "         [0.7700, 0.2500, 0.1000],\n",
            "         [0.0500, 0.8000, 0.5500]],\n",
            "\n",
            "        [[0.4300, 0.1500, 0.8900],\n",
            "         [0.5500, 0.8700, 0.6600],\n",
            "         [0.5700, 0.8500, 0.6400],\n",
            "         [0.2200, 0.5800, 0.3300],\n",
            "         [0.7700, 0.2500, 0.1000],\n",
            "         [0.0500, 0.8000, 0.5500]]])\n",
            "torch.Size([2, 6, 3])\n",
            "Causal Attention Outputs\n",
            "tensor([[[0.4566, 0.2729],\n",
            "         [0.5792, 0.3011],\n",
            "         [0.6249, 0.3102],\n",
            "         [0.5691, 0.2785],\n",
            "         [0.5543, 0.2520],\n",
            "         [0.5337, 0.2499]],\n",
            "\n",
            "        [[0.4566, 0.2729],\n",
            "         [0.5792, 0.3011],\n",
            "         [0.6249, 0.3102],\n",
            "         [0.5691, 0.2785],\n",
            "         [0.5543, 0.2520],\n",
            "         [0.5337, 0.2499]]], grad_fn=<UnsafeViewBackward0>)\n",
            "torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `MultiHeadCausalAttentionWrapper`: Multi Head Attention with separate `CausalSelfAttention` heads"
      ],
      "metadata": {
        "id": "WLZbzscs-ix7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadCausalAttentionWrapper(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, drop_rate, num_heads, qkv_bias=False):\n",
        "    super(MultiHeadCausalAttentionWrapper, self).__init__()\n",
        "    self.heads = nn.ModuleList([CausalSelfAttention(d_in, d_out, context_length, drop_rate, qkv_bias=False)\n",
        "                                for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    Here we can use torch.bmm\n",
        "    '''\n",
        "    return torch.cat([h(x) for h in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "_Bscf5IxcNVX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mhsa_w = MultiHeadCausalAttentionWrapper(d_in, d_out, batch.shape[1], 0.0, 2)\n",
        "mhsa_w_outputs = mhsa_w(batch)\n",
        "print('Multi Head Causal Self Attention Wrapper Outputs')\n",
        "print(mhsa_w_outputs)\n",
        "print(mhsa_w_outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBJy1aWJpLnu",
        "outputId": "8bac07c2-e52b-4e98-fb05-015cb20004aa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi Head Causal Self Attention Wrapper Outputs\n",
            "tensor([[[-0.5684,  0.5063, -0.4821,  0.4336],\n",
            "         [-0.5388,  0.6447, -0.5368,  0.5483],\n",
            "         [-0.5242,  0.6954, -0.5545,  0.5886],\n",
            "         [-0.4578,  0.6471, -0.4937,  0.5311],\n",
            "         [-0.4006,  0.5921, -0.4589,  0.5169],\n",
            "         [-0.3997,  0.5971, -0.4479,  0.4971]],\n",
            "\n",
            "        [[-0.5684,  0.5063, -0.4821,  0.4336],\n",
            "         [-0.5388,  0.6447, -0.5368,  0.5483],\n",
            "         [-0.5242,  0.6954, -0.5545,  0.5886],\n",
            "         [-0.4578,  0.6471, -0.4937,  0.5311],\n",
            "         [-0.4006,  0.5921, -0.4589,  0.5169],\n",
            "         [-0.3997,  0.5971, -0.4479,  0.4971]]], grad_fn=<CatBackward0>)\n",
            "torch.Size([2, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mhsa_w1 = MultiHeadCausalAttentionWrapper(d_in, 1, batch.shape[1], 0.0, 2)\n",
        "mhsa_w1_outputs = mhsa_w1(batch)\n",
        "print('New Multi Head Causal Self Attention Wrapper Outputs')\n",
        "print(mhsa_w1_outputs)\n",
        "print(mhsa_w1_outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY-BdIFTqAuZ",
        "outputId": "7c55aefe-b316-4ba1-cc74-7be5a8e7be43"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Multi Head Causal Self Attention Wrapper Outputs\n",
            "tensor([[[0.7128, 0.4106],\n",
            "         [0.8309, 0.3569],\n",
            "         [0.8696, 0.3342],\n",
            "         [0.7802, 0.2922],\n",
            "         [0.7388, 0.2238],\n",
            "         [0.7163, 0.2381]],\n",
            "\n",
            "        [[0.7128, 0.4106],\n",
            "         [0.8309, 0.3569],\n",
            "         [0.8696, 0.3342],\n",
            "         [0.7802, 0.2922],\n",
            "         [0.7388, 0.2238],\n",
            "         [0.7163, 0.2381]]], grad_fn=<CatBackward0>)\n",
            "torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `MultiHeadSelfAttention`: Efficient Multi Head Self Attention with batch matrix multiplication"
      ],
      "metadata": {
        "id": "QJYuLegQ-4T8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "why don't we directly use\n",
        "\n",
        "```\n",
        "value = query.view(num_seq, self.num_heads, num_tokens, self.head_dim)\n",
        "```\n",
        "in this way, number of transposes will be less\n",
        "\n",
        "1. `tensor.contiguous()` creates a tensor with the same memory mapping as if it is created from scratch.\n",
        "2. `tensor.view()` or `tensor.transpose()` or `tensor.reshape()` modifies this mapping.\n",
        "3. also, `view/reshape` is not the same as `transpose`. we can verify this by using `flatten` or `view(-1)`.\n",
        "\n",
        "Reference: [stackoverflow](https://stackoverflow.com/questions/48915810/what-does-contiguous-do-in-pytorch)\n"
      ],
      "metadata": {
        "id": "1eNDLc4YyojA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, num_heads, drop_rate, qkv_bias=False):\n",
        "    super(MultiHeadSelfAttention, self).__init__()\n",
        "\n",
        "    self.in_dim = d_in\n",
        "    self.out_dim = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = self.out_dim//self.num_heads\n",
        "    self.context_length = context_length\n",
        "\n",
        "    self.W_query = nn.Linear(self.in_dim, self.out_dim, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(self.in_dim, self.out_dim, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(self.in_dim, self.out_dim, bias=qkv_bias)\n",
        "    # combines the outputs from all heads\n",
        "    self.out_proj = nn.Linear(self.out_dim, self.out_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(drop_rate)\n",
        "    self.register_buffer('mask',\n",
        "                         torch.triu(torch.ones(self.context_length, self.context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    num_seq, num_tokens, _ = x.shape\n",
        "\n",
        "    query = self.W_query(x)\n",
        "    key = self.W_key(x)\n",
        "    value = self.W_value(x)\n",
        "\n",
        "    query = query.view(num_seq, num_tokens, self.num_heads, self.head_dim)\n",
        "    key = query.view(num_seq, num_tokens, self.num_heads, self.head_dim)\n",
        "    value = query.view(num_seq, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    attention_scores = query.transpose(1, 2) @ key.transpose(1, 2).transpose(2, 3)   # --> num_seq, num_heads, num_tokens, num_tokens\n",
        "    attention_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "    attention_weights = torch.softmax(attention_scores/self.head_dim ** 0.5, dim=-1)\n",
        "    drop_attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "    context_vector = drop_attention_weights @ value.transpose(1, 2) # --> num_seq, num_heads, num_tokens, head_dim\n",
        "    context_vector = context_vector.transpose(1, 2) # --> num_seq, num_tokens, num_heads, head_dim --> transpose is not the same as view/reshape\n",
        "    # --> create same memory mapping as if created from scratch\n",
        "    context_vector = context_vector.contiguous().view(num_seq, num_tokens, self.out_dim) # --> num_seq, num_tokens, out_dim\n",
        "    context_vector = self.out_proj(context_vector) # --> num_seq, num_tokens, out_dim\n",
        "    return context_vector"
      ],
      "metadata": {
        "id": "MdKAwhM_qkcw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mhsa = MultiHeadSelfAttention(d_in, d_out, batch.shape[1], 2, 0.0)\n",
        "mhsa_outputs = mhsa(batch)\n",
        "print('Multi Head Self Attention Outputs')\n",
        "print(mhsa_outputs)\n",
        "print(mhsa_outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNMwESeb1WE9",
        "outputId": "421e2738-5ede-4866-9219-ab3a74898dee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi Head Self Attention Outputs\n",
            "tensor([[[-0.1700,  0.2093],\n",
            "         [-0.1529,  0.2570],\n",
            "         [-0.1486,  0.2705],\n",
            "         [-0.1606,  0.2722],\n",
            "         [-0.1784,  0.2464],\n",
            "         [-0.1737,  0.2648]],\n",
            "\n",
            "        [[-0.1700,  0.2093],\n",
            "         [-0.1529,  0.2570],\n",
            "         [-0.1486,  0.2705],\n",
            "         [-0.1606,  0.2722],\n",
            "         [-0.1784,  0.2464],\n",
            "         [-0.1737,  0.2648]]], grad_fn=<ViewBackward0>)\n",
            "torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`MultiHeadCausalAttentionWrapper` vs `MultiHeadSelfAttention`:\n",
        "\n",
        "- wrapper goes through each head sequentially so it is slower. the combined class implements all computations in parallel so it is faster.\n",
        "- wrapper stacks individual causal attention heads. the combined class uses a batched matrix multiplication.\n",
        "- wrapper has a separate `W_query`, `W_key`, and `W_value` matrix for each head. So the input parameter `d_out` will be `d_out//num_heads` so that the final output dim is `d_out`. the combined class implements one `W_query`, `W_key`, and `W_value` matrix for all heads and splits their outputs into as many vectors as there are heads. So the input parameter `d_out` will be `d_out` only.\n",
        "- For `W_query`, `W_key`, and `W_value` matrices, both implementations have the same number of parameters.\n",
        "- In either implementation, we have not scaled the context vector after the application of dropout to account for the effect of dropped neurons.\n",
        "\n",
        "Note: `torch.bmm()` works with 3D tensors only, whereas `@` works with tensors of any shape as long as the last two dimensions permit matrix multiplication."
      ],
      "metadata": {
        "id": "JMM9VwIU2yAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT2 Self Attention"
      ],
      "metadata": {
        "id": "F3aYDQYs_OBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_context_length = 1024\n",
        "gpt2_embedding_dim = 768\n",
        "gpt2_attention_heads = 12\n",
        "\n",
        "gpt2_mhsa_w1 = MultiHeadCausalAttentionWrapper(gpt2_embedding_dim,\n",
        "                                               gpt2_embedding_dim//gpt2_attention_heads,\n",
        "                                               gpt2_context_length,\n",
        "                                               0.1, gpt2_attention_heads)\n",
        "gpt2_mhsa = MultiHeadSelfAttention(gpt2_embedding_dim,\n",
        "                                   gpt2_embedding_dim,\n",
        "                                   gpt2_context_length,\n",
        "                                   gpt2_attention_heads, 0.1)\n",
        "\n",
        "batch = torch.rand(size=(10, 512, 768)) # --> improper batch shape (previous shape was [2, 6, 3]) was causing error in this step"
      ],
      "metadata": {
        "id": "LomxYQqz4SYX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "mhsa_w1_outputs = gpt2_mhsa_w1(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naW2EcGE4IAl",
        "outputId": "07db6d23-e5db-4ce1-a741-22d33dd3fce3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 911 ms, sys: 998 ms, total: 1.91 s\n",
            "Wall time: 2.28 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "mhsa_outputs = gpt2_mhsa(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inUr1-Ba4PAe",
        "outputId": "6de09fe4-e535-43a8-859c-de23173efdf7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 773 ms, sys: 1.07 s, total: 1.84 s\n",
            "Wall time: 1.83 s\n"
          ]
        }
      ]
    }
  ]
}
