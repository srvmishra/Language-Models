{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmp8weuVudgGHqanWDfa0c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srvmishra/Language-Models/blob/main/GPT2_Model_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We-KhWGzV1D3",
        "outputId": "7f77d754-788d-4770-bd65-27b63a7844ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "c1uxLHiohLXG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PN1m_sJtYHov"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`cfg` is a dictionary whose fields indicate properties of the LLM\n",
        "- `vocab_size` - vocabulary size\n",
        "- `context_length` - context length\n",
        "- `emb_dim` - embedding dimension - input as well as output of transformer blocks\n",
        "- `n_heads` - number of attention heads inside a transformer block\n",
        "- `n_layers` - number of transformer blocks\n",
        "- `drop_rate` - dropout rate\n",
        "- `qkv_bias` - whether to include query-key-value bias"
      ],
      "metadata": {
        "id": "vX6WHxeiZXyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT2 LLM architecture:\n",
        "\n",
        "token embedding + position embedding -> dropout -> transformer blocks -> layer norm -> output head -> logits\n",
        "\n",
        "1. why `qkv_bias` is disabled in LLMs?\n",
        "2. why a dropout is used just before the transformer blocks?"
      ],
      "metadata": {
        "id": "t2jxQg0mgF0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer Normalization"
      ],
      "metadata": {
        "id": "0hhZkNSAhOm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why Layer Normalization?\n",
        "1. Batch norm computes statistics across batch dimension so token representations from different sequences might mix resulting in undesirable behaviour.\n",
        "2. Usually there are different number of tokens in each sequence and we do not want to consider the representations of padding tokens into calculation.\n",
        "3. Since LLMs are parameter heavy, batch size for training and testing might be different due to hardware requirements. Layer norm avoids this inconsistency by normalizing each input feature dimension independently."
      ],
      "metadata": {
        "id": "k7P2EUbZiCcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super(LayerNorm, self).__init__()\n",
        "    self.eps = 1e-5\n",
        "\n",
        "    # learnable scale and shift parameters\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # calculate mean and variance along emb_dim (feature dimension)\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "\n",
        "    # normalize input --> scale and shift it to return output\n",
        "    normalized_x = (x - mean)/torch.sqrt(var + self.eps)\n",
        "    return self.scale * normalized_x + self.shift"
      ],
      "metadata": {
        "id": "RHpPayn1gFE9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GELU activation\n",
        "\n",
        "$GELU(x) \\approx 0.5\\cdot x\\cdot(1 + \\tanh[\\sqrt{\\dfrac{2}{\\pi}}\\cdot(x + 0.044715\\cdot x^3)])$"
      ],
      "metadata": {
        "id": "PxC1jqkEjQdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GELU, self).__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * (1.0 + torch.tanh(\n",
        "        torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715*x**3)\n",
        "        ))"
      ],
      "metadata": {
        "id": "CmtYpkUqgEWI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feed Forward Network\n",
        "\n",
        "Learn and generalize based on the specific patterns in the dataset."
      ],
      "metadata": {
        "id": "YD-DYI-pk8VE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super(FeedForward, self).__init__()\n",
        "\n",
        "    self.layers = nn.Sequential(nn.Linear(cfg['emb_dim'], 4 * cfg['emb_dim']),\n",
        "                                GELU(),\n",
        "                                nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim']))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "Pcd2aHE1kqOA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masked Multi Head Self Attention\n",
        "\n",
        "This was implemented in `Self_Attention_Mechanism.ipynb`, so we simply take it from there."
      ],
      "metadata": {
        "id": "Z8IK3QiooLfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, num_heads, drop_rate, qkv_bias=False):\n",
        "    super(MultiHeadSelfAttention, self).__init__()\n",
        "\n",
        "    self.in_dim = d_in\n",
        "    self.out_dim = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = self.out_dim//self.num_heads\n",
        "    self.context_length = context_length\n",
        "\n",
        "    self.W_query = nn.Linear(self.in_dim, self.out_dim, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(self.in_dim, self.out_dim, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(self.in_dim, self.out_dim, bias=qkv_bias)\n",
        "    # combines the outputs from all heads\n",
        "    self.out_proj = nn.Linear(self.out_dim, self.out_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(drop_rate)\n",
        "    self.register_buffer('mask',\n",
        "                         torch.triu(torch.ones(self.context_length, self.context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    num_seq, num_tokens, _ = x.shape\n",
        "\n",
        "    query = self.W_query(x)\n",
        "    key = self.W_key(x)\n",
        "    value = self.W_value(x)\n",
        "\n",
        "    query = query.view(num_seq, num_tokens, self.num_heads, self.head_dim)\n",
        "    key = query.view(num_seq, num_tokens, self.num_heads, self.head_dim)\n",
        "    value = query.view(num_seq, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    attention_scores = query.transpose(1, 2) @ key.transpose(1, 2).transpose(2, 3)   # --> num_seq, num_heads, num_tokens, num_tokens\n",
        "    attention_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "    attention_weights = torch.softmax(attention_scores/self.head_dim ** 0.5, dim=-1)\n",
        "    drop_attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "    context_vector = drop_attention_weights @ value.transpose(1, 2) # --> num_seq, num_heads, num_tokens, head_dim\n",
        "    context_vector = context_vector.transpose(1, 2) # --> num_seq, num_tokens, num_heads, head_dim --> transpose is not the same as view/reshape\n",
        "    # --> create same memory mapping as if created from scratch\n",
        "    context_vector = context_vector.contiguous().view(num_seq, num_tokens, self.out_dim) # --> num_seq, num_tokens, out_dim\n",
        "    context_vector = self.out_proj(context_vector) # --> num_seq, num_tokens, out_dim\n",
        "    return context_vector"
      ],
      "metadata": {
        "id": "88cRmMcpm15l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Block Architecture\n",
        "\n",
        "Connects all the above components into one block.\n",
        "\n",
        "(input -> Layer Norm 1 -> Masked Multi Head Self Attention -> Dropout) -> (Layer Norm 2 -> Feed Forward Network -> Dropout) -> output\n",
        "\n",
        "input and output dimensions are the same so that multiple blocks can be stacked on top of each other. (block) indicates a residual connection between input and output of the block in the above architecture.\n",
        "\n",
        "each operation is applied position wise on every sequence. that is, each each token in each sequence goes through each of the above operations. tokens from one sequence interact with each other only through the self attention mechanism. other operations are applied at all token positions independently of others."
      ],
      "metadata": {
        "id": "T4kG_AKaob2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.att = MultiHeadSelfAttention(cfg['emb_dim'], cfg['emb_dim'],\n",
        "                                      cfg['context_length'], cfg['n_heads'],\n",
        "                                      cfg['drop_rate'], cfg['qkv_bias'])\n",
        "\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.norm1 = LayerNorm(cfg['emb_dim'])\n",
        "    self.norm2 = LayerNorm(cfg['emb_dim'])\n",
        "    self.drop_shortcut = nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.drop_shortcut(self.att(self.norm1(x)))\n",
        "    x = x + self.drop_shortcut(self.ff(self.norm2(x)))\n",
        "    return x"
      ],
      "metadata": {
        "id": "rSJI3Ueum1yK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT2 architecture\n",
        "\n",
        "token embedding + position embedding -> dropout -> transformer blocks -> layer norm -> output head -> logits\n",
        "\n",
        "Combining all the above modules into the GPT2 architecture.\n",
        "\n",
        "1. Tokenizer takes care of generating token ids from text.\n",
        "2. Token ids are converted into token embeddings and position embeddings are added to it in the first part of GPT2.\n",
        "3. A dropout is then applied.\n",
        "4. The resuling tensor is passed through the transformer blocks.\n",
        "5. Next, layer norm, and output head follow sequentially.\n",
        "\n",
        "for the stability of learning, there are a lot of skip connections, and layer normalizations inside the GPT2 architecture.\n",
        "\n",
        "why there is no bias in the final output layer?"
      ],
      "metadata": {
        "id": "WrdMYsVt5rZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Model(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super(GPT2Model, self).__init__()\n",
        "\n",
        "    self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
        "    self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
        "    self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
        "    self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
        "    # * unpacks the list as nn.Sequential expects, and use range while iterating with for loop\n",
        "    self.final_norm = LayerNorm(cfg['emb_dim'])\n",
        "    self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
        "\n",
        "  def forward(self, x): # --> batch_size, num_tokens\n",
        "    tok_emb = self.tok_emb(x)\n",
        "    # keep 0 inside torch.arange, add device\n",
        "    pos_emb = self.pos_emb(torch.arange(0, x.shape[1]))#, device=x.device)\n",
        "    # device throws error: `embedding forward got an unexpected keyword argument device`\n",
        "    x = tok_emb + pos_emb\n",
        "\n",
        "    x = self.drop_emb(x)\n",
        "\n",
        "    x = self.trf_blocks(x)\n",
        "\n",
        "    x = self.final_norm(x)\n",
        "    x = self.out_head(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "yiN7CnQG7HwM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Number of parameters in GPT2 model.\n",
        "\n",
        "GPT2 uses *weight_tying* - the weights of the token embedding layer and the output head are the same.\n",
        "\n",
        "Modern LLMs use different weights for these layers for better training and model performance like we have used here."
      ],
      "metadata": {
        "id": "iQPlzwwN_w3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT2_CONFIG_124M = {'vocab_size': 50257,\n",
        "                    'context_length': 1024,\n",
        "                    'emb_dim': 768,\n",
        "                    'n_heads': 12,\n",
        "                    'n_layers': 12,\n",
        "                    'drop_rate': 0.1,\n",
        "                    'qkv_bias': False}\n",
        "\n",
        "GPT2_MEDIUM = {'vocab_size': 50257,\n",
        "               'context_length': 1024,\n",
        "               'emb_dim': 1024,\n",
        "               'n_heads': 16,\n",
        "               'n_layers': 24,\n",
        "               'drop_rate': 0.1,\n",
        "               'qkv_bias': False}\n",
        "\n",
        "GPT2_LARGE = {'vocab_size': 50257,\n",
        "              'context_length': 1024,\n",
        "              'emb_dim': 1280,\n",
        "              'n_heads': 20,\n",
        "              'n_layers': 36,\n",
        "              'drop_rate': 0.1,\n",
        "              'qkv_bias': False}\n",
        "\n",
        "GPT2_XL = {'vocab_size': 50257,\n",
        "           'context_length': 1024,\n",
        "           'emb_dim': 1600,\n",
        "           'n_heads': 25,\n",
        "           'n_layers': 48,\n",
        "           'drop_rate': 0.1,\n",
        "           'qkv_bias': False}"
      ],
      "metadata": {
        "id": "j-SsNYCkKw-1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_num_params(gpt_model, exclude_output=False):\n",
        "  size_mb = lambda x: (4 * x)/(1024 ** 2)\n",
        "  params = lambda x: sum(p.numel() for p in x)\n",
        "\n",
        "  # tok_emb = gpt_model.tok_emb.parameters().numel()\n",
        "  # pos_emb = gpt_model.pos_emb.parameters().numel()\n",
        "  # trf_blocks = gpt_model.trf_blocks().parameters().numel()\n",
        "  # out_head = gpt_model.out_head.parameters().numel()\n",
        "\n",
        "  tok_emb = params(gpt_model.tok_emb.parameters())\n",
        "  pos_emb = params(gpt_model.pos_emb.parameters())\n",
        "  trf_blocks = params(gpt_model.trf_blocks.parameters())\n",
        "  out_head = params(gpt_model.out_head.parameters())\n",
        "\n",
        "  if exclude_output:\n",
        "    total = tok_emb + pos_emb + trf_blocks\n",
        "  else:\n",
        "    total = tok_emb + pos_emb + trf_blocks + out_head\n",
        "\n",
        "  print('Trainable Parameters ... ')\n",
        "  print(f'Token Embedding layer: {tok_emb}, size: {size_mb(tok_emb)} MB')\n",
        "  print(f'Position Embedding layer: {pos_emb}, size: {size_mb(pos_emb)} MB')\n",
        "  print(f'Transformer Blocks: {trf_blocks}, size: {size_mb(trf_blocks)} MB')\n",
        "  print(f'Output Head: {out_head}, size: {size_mb(out_head)} MB')\n",
        "  print(f'Total params: {total}, size: {size_mb(total)} MB')\n",
        "  print('###################################################')\n",
        "\n",
        "  print('Trainable parameters in attention and feed forward blocks ...')\n",
        "  trf_blk = gpt_model.trf_blocks[0]\n",
        "  num_trf_blks = len(gpt_model.trf_blocks)\n",
        "  # attn = trf_blk.att.parameters().numel() * num_trf_blks\n",
        "  # ffns = trf_blk.ff.parameters().numel() * num_trf_blks\n",
        "  attn = params(trf_blk.att.parameters()) * num_trf_blks\n",
        "  ffns = params(trf_blk.ff.parameters()) * num_trf_blks\n",
        "  print(f'Attention Layer Parameters: {attn}, size: {size_mb(attn)} MB')\n",
        "  print(f'Feed Forward Network Parameters: {ffns}, size: {size_mb(ffns)} MB')\n",
        "  print('###################################################')\n",
        "  return"
      ],
      "metadata": {
        "id": "op0U_C2I_ZVu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_small = GPT2Model(GPT2_CONFIG_124M)\n",
        "calculate_num_params(gpt2_small)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgrNIha3_ZQ9",
        "outputId": "a1fbda9a-5dda-450c-dc89-82c444cc0408"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable Parameters ... \n",
            "Token Embedding layer: 38597376, size: 147.2373046875 MB\n",
            "Position Embedding layer: 786432, size: 3.0 MB\n",
            "Transformer Blocks: 85026816, size: 324.3515625 MB\n",
            "Output Head: 38597376, size: 147.2373046875 MB\n",
            "Total params: 163008000, size: 621.826171875 MB\n",
            "###################################################\n",
            "Trainable parameters in attention and feed forward blocks ...\n",
            "Attention Layer Parameters: 28320768, size: 108.03515625 MB\n",
            "Feed Forward Network Parameters: 56669184, size: 216.17578125 MB\n",
            "###################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_medium = GPT2Model(GPT2_MEDIUM)\n",
        "calculate_num_params(gpt2_medium)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXRGtiyP_ZMF",
        "outputId": "c16b5ecf-1104-424d-8f74-fb4606a25099"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable Parameters ... \n",
            "Token Embedding layer: 51463168, size: 196.31640625 MB\n",
            "Position Embedding layer: 1048576, size: 4.0 MB\n",
            "Transformer Blocks: 302235648, size: 1152.9375 MB\n",
            "Output Head: 51463168, size: 196.31640625 MB\n",
            "Total params: 406210560, size: 1549.5703125 MB\n",
            "###################################################\n",
            "Trainable parameters in attention and feed forward blocks ...\n",
            "Attention Layer Parameters: 100687872, size: 384.09375 MB\n",
            "Feed Forward Network Parameters: 201449472, size: 768.46875 MB\n",
            "###################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_large = GPT2Model(GPT2_LARGE)\n",
        "calculate_num_params(gpt2_large)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-pSUzeLMXX3",
        "outputId": "9538bfc6-2389-43a0-dc92-aba3499b7dd4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable Parameters ... \n",
            "Token Embedding layer: 64328960, size: 245.3955078125 MB\n",
            "Position Embedding layer: 1310720, size: 5.0 MB\n",
            "Transformer Blocks: 708249600, size: 2701.7578125 MB\n",
            "Output Head: 64328960, size: 245.3955078125 MB\n",
            "Total params: 838218240, size: 3197.548828125 MB\n",
            "###################################################\n",
            "Trainable parameters in attention and feed forward blocks ...\n",
            "Attention Layer Parameters: 235975680, size: 900.17578125 MB\n",
            "Feed Forward Network Parameters: 472089600, size: 1800.87890625 MB\n",
            "###################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gpt2_xl = GPT2Model(GPT2_XL) # --> system crashes due to limited RAM\n",
        "# calculate_num_params(gpt2_xl)"
      ],
      "metadata": {
        "id": "P6MVpy7VMYFv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Generation with GPT2 Models\n",
        "\n",
        "text -> (`tokens_ids` -> GPT Model -> `next_token_id`) -> end of generation\n",
        "\n",
        "append `next_token_id` to `token_ids` and keep generating till a specified number of tokens is reached.\n",
        "\n",
        "`next_token_id` is sampled from the last output of the GPT Model by taking the token in the vocabulary that has the maximum probability.\n",
        "\n",
        "in the implementation below, we did not check for two conditions:\n",
        "1. generating beyond `context_length` - assume that `num_tokens_to_generate` << `context_length`.\n",
        "2. checking for `EOS` token to stop generation - assume that model keeps generating `EOS` till `num_tokens_to_generate` is reached.\n",
        "\n",
        "the outputs will not make sense because the models are not trained.\n",
        "\n",
        "also, everytime a new model is created, its parameters are initialized again. since each time we have different parameters, we will get different outputs, as we can see by running the cells below repeatedly."
      ],
      "metadata": {
        "id": "6BYGvrAsMnCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(gpt_model, tokenizer, input_text, context_length, num_tokens_to_generate=20):\n",
        "  input_ids = torch.tensor(tokenizer.encode(input_text))\n",
        "\n",
        "  if len(input_ids.shape) == 1:\n",
        "    input_ids = input_ids.unsqueeze(0)\n",
        "    # print(input_ids.shape)\n",
        "\n",
        "  for _ in range(num_tokens_to_generate):\n",
        "    input_ids_trunc = input_ids[:, -context_length:] # --> we want to keep generating, so we keep the latest tokens only,\n",
        "    # so we do not use [:, :context_length], as it keeps the same old tokens - to the first `context_length` tokens\n",
        "    with torch.no_grad():\n",
        "      model_outs = gpt_model(input_ids)[:, -1, :]\n",
        "    probs = torch.softmax(model_outs, dim=-1)\n",
        "    next_token_id = torch.argmax(probs, dim=-1, keepdim=True) # --> keepdim is specified so that concatenation will be easy\n",
        "    input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
        "\n",
        "  input_ids = input_ids.cpu().numpy() # can this tokenizer encode & decode a batch of text sequences?\n",
        "  # no, so we use a loop over all sequences\n",
        "  for tid in input_ids:\n",
        "    # print(tid.shape)\n",
        "    print(tokenizer.decode(tid.tolist()))\n",
        "  return"
      ],
      "metadata": {
        "id": "H2vpLU6fMrJj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "input_text = 'Hello, I am '\n",
        "num_tokens_to_generate = 20\n",
        "context_length = 1024"
      ],
      "metadata": {
        "id": "U_M0XUAPOU5H"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text_simple(gpt2_small.eval(), tokenizer, input_text, context_length, num_tokens_to_generate=num_tokens_to_generate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCdWLmuHUz1j",
        "outputId": "ef92f2fb-c619-4bb3-ecf6-405683d87928"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, I am  spawned OLEDcomedPat Spoiler orthodox displayonut substit attendedletters constitutional Baptist Launcher commonplacetion vaginal comma Sam Closed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text_simple(gpt2_medium.eval(), tokenizer, input_text, context_length, num_tokens_to_generate=num_tokens_to_generate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E486AwiKVA52",
        "outputId": "96450665-8d42-434a-f9fd-590439e69b96"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, I am asmajet riddled Sty sportsKill Iro Thumbnails acceptable allegation preserved Squid Node teenager influounced abbre   Most overturn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text_simple(gpt2_large.eval(), tokenizer, input_text, context_length, num_tokens_to_generate=num_tokens_to_generate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOrsOtQPVA0u",
        "outputId": "9900fb9e-116a-4ce2-c76a-e8017c324549"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, I am  bere Taste unlimitedAnyoneaq Courtney Absolute NY bubbles frequently portableFormer Joberedithfoliosignantdream 1975 mand harsh\n"
          ]
        }
      ]
    }
  ]
}